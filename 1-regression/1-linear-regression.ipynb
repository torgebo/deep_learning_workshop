{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This area sets up the Jupyter environment.\n",
    "Please do not modify anything in this cell.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project to PYTHONPATH for future use\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "# Import miscellaneous modules\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Set CSS styling\n",
    "with open('../admin/custom.css', 'r') as f:\n",
    "    style = \"\"\"<style>\\n{}\\n</style>\"\"\".format(f.read())\n",
    "    display(HTML(style))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "The following notebook will go through the basics of **supervised learning**.\n",
    "</div>\n",
    "\n",
    "In supervised learning we assume that our data consist of **input - output** pairs. A learning algorithm analyses the data and produces a function, or model, we can use to infer *outputs* given unseen future *inputs*.\n",
    "\n",
    "Below we can see a simplified illustration of the supervised learning problem.\n",
    "\n",
    "Pairs of inputs $\\mathbf{x}$ and outputs $y$ constitutes our training examples, where the inputs are sampled from a probability distribution. A pair $(\\mathbf{x}, y)$ is related by an *unknown* target function $f$ governed by a conditional probability distribution. The ultimate goal of supervised learning is to learn a function $g$ which approximates $f$ well.\n",
    "\n",
    "The particular approximation $g$ we pick is called a hypothesis. A learning algorithm is responsible for picking the most appropriate hypothesis from a hypothesis set. The decision between which hypothesis to pick is done by looking at the *data* and typically involves an error function which measures how good a hypothesis may be.\n",
    "\n",
    "<img src=\"resources/supervised-learning.png\" alt=\"Supervised Learning\" width=\"700\" />\n",
    "\n",
    "When our learning algorithm has picked a good hypothesis, we can feed it new and unseen samples to produce output estimates.\n",
    "\n",
    "The name of the data typically differ depending on which area you are from.\n",
    "\n",
    "The **input** variables are commonly known as:\n",
    "\n",
    "    - covariates\n",
    "    - predictors\n",
    "    - features \n",
    "\n",
    "The **output** variables are commonly known as:\n",
    "\n",
    "    - variates\n",
    "    - targets\n",
    "    - labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models: Regression\n",
    "\n",
    "For now we will focus on one of the simplest supervised learning problems: *linear regression*.\n",
    "\n",
    "A linear regression model learns a real-valued function where one or more dependent output variable(s) *depend* linearly on one or more independent input variable(s). Geometrically, this real-valued function can be interpreted as a hyperplane which we attempt to fit to our data.\n",
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "* Allows us to investigate the relationship between two or more variables statistically\n",
    "* Can be thought of as a building block of artificial neural networks\n",
    "* A solution can be found analytically or using data-driven optimisation\n",
    "* Basic introduction to supervised learning\n",
    "* Introduces you to the Python programming language and Jupyter notebook usage\n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/machine_learning.png\" alt=\"xkcd\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "This notebook will use the following notation:\n",
    "\n",
    "* A (training) dataset has $N$ input - output pairs: $(\\mathbf{x}_i, y_i)$, where $i$ signifies the $i$th example\n",
    "* Each input $\\mathbf{x}_i$ is a $d$ dimensional column vector: $\\mathbf{x}_i \\in \\mathbb{R}^d$\n",
    "* For this notebook we will assume the output to be univariate: $y \\in \\mathbb{R}$\n",
    "\n",
    "Keep in mind that additional notation will be introduced as we continue through the notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Income vs. Education\n",
    "\n",
    "In the following example we will load data from a CSV file and use to estimate a linear model between an `Education index` and a `Income index`.\n",
    "\n",
    "* **input** $\\rightarrow$ Scalar metric indicating level of education\n",
    "* **output** $\\rightarrow$ Scalar metric indication level of income\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>In the follow code snippets we will:</strong>\n",
    "  <ul>\n",
    "    <li>Load data from a CSV file</li>\n",
    "    <li>Plot the data</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "First, let's begin by importing a selection of Python package that will prove useful for the rest of this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plots will be show inside the notebook\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy is a package for manipulating N-dimensional array objects \n",
    "import numpy as np\n",
    "\n",
    "# Pandas is a data analysis package\n",
    "import pandas as pd\n",
    "\n",
    "import problem_unittests as tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Pandas we can load the aforementioned CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and print the first n = 5 rows\n",
    "# URL: http://www-bcf.usc.edu/~gareth/ISL/Income1.csv\n",
    "DATA_URL = './resources/Income1.csv'\n",
    "data = pd.read_csv(DATA_URL, index_col=0)\n",
    "\n",
    "print(data.head(n=5))\n",
    "\n",
    "# Put the second (education index) and third (income index) row in a NumPy array\n",
    "X_data = data['Education'].values\n",
    "y_data = data['Income'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded we can plot it as a scatter plot using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.scatter(X_data, y_data, label='Training data')\n",
    "\n",
    "plt.title('Education vs. Income')\n",
    "plt.xlabel('Education index')\n",
    "plt.ylabel('Income index')\n",
    "plt.grid(linestyle='dotted')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "As previously mentioned, we will be using a linear model. That is, the output will be a linear combination of the input plus a bias or intercept:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "g(\\mathbf{x}) = b + \\sum_{j=1}^{d}w_jx_j\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Keep in mind that in this problem there is only a single independent variable $\\mathbf{x}$, which means the above can be simplified to: $g(x) = b + wx$, where $b$ is the intercept and $w$ is the slope.\n",
    "\n",
    "\n",
    "### Notational Simplifications\n",
    "\n",
    "To simplify notation, it is quite common to merge the bias $b$ with the weights $w_i$ to get a single weight vector $\\mathbf{w} = (w_0, w_1, \\ldots, w_d)^\\intercal$, where $w_0 = b$. Consequently, an extra dimension must be prepended to the input vector, i.e. $\\mathbf{x} = (1, x_1, \\ldots, x_d)^\\intercal$.\n",
    "\n",
    "With this simplification the linear model can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "g(\\mathbf{x}) = \\sum_{j=1}^{d}w_jx_j\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Matrix Form\n",
    "\n",
    "The above model takes a single input $\\mathbf{x}$ and produces a single output prediction. We can take this one step further by putting all of the input examples in a single matrix called the *design matrix* $\\mathbf{X}$. This matrix consists of one (training) example per row.\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "  1 & \\mathbf{x}_{11} & \\cdots & \\mathbf{x}_{1d} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  1 & \\mathbf{x}_{N1} & \\cdots & \\mathbf{x}_{Nd}\n",
    "\\end{bmatrix} =\n",
    "\\left[ \\begin{array}{c} \\mathbf{x}_{1}^\\intercal \\\\ \\vdots\\\\ \\mathbf{x}_{N}^\\intercal\\end{array} \\right]\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "\n",
    "With the design matrix, predictions can be done by matrix multiplication:\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w} =\n",
    "\\begin{bmatrix}\n",
    "  1 & \\mathbf{x}_{11} & \\cdots & \\mathbf{x}_{1d} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  1 & \\mathbf{x}_{N1} & \\cdots & \\mathbf{x}_{Nd}\n",
    "\\end{bmatrix}\n",
    "\\left[ \\begin{array}{c} \\mathbf{w}_{0} \\\\ \\mathbf{w}_{1} \\\\ \\vdots\\\\ \\mathbf{w}_{d}\\end{array} \\right] =\n",
    "\\left[ \\begin{array}{c} y_{1} \\\\ y_{2} \\\\ \\vdots\\\\ y_{N}\\end{array} \\right]\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an Error Function\n",
    "\n",
    "To measure how well our hypothesis, i.e. a particular set of weights, approximates the unknown target function $f$ we will have to come up with an error function. This quantification, which we will call $J$, goes by several different names:\n",
    "\n",
    "* Cost\n",
    "* Energy\n",
    "* Error\n",
    "* Loss\n",
    "* Objective\n",
    "\n",
    "We will be using *squared error*: $(g(\\mathbf{x}) - f(\\mathbf{x}))^2$ to measure how well our hypothesis approximates $f$. Seeing as we do not have access to $f$ we will instead compute an in-sample squared error over all our training data. This measure is commonly known as *mean squared error* (MSE):\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "J(\\mathbf{w}) =\n",
    "\\frac{1}{N}\\sum_{i=1}^{N}(g(\\mathbf{x}_i) - y_i)^2 =\n",
    "\\frac{1}{N}\\sum_{i=1}^{N}(\\mathbf{w}^\\intercal \\mathbf{x}_i - y_i)^2 =\n",
    "\\frac{1}{N}\\lVert \\mathbf{X}\\mathbf{w} - \\mathbf{y} \\rVert^2\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "A simple analogy is to think of mean squared error as a set of springs, one per training example. The objective of the learning algorithm is to balance the learned hyperplane by attempting to push it as close as we can to each of the training samples. Thus, the futher the training sample is to our hyperplane, the stronger the force is on a particular spring.\n",
    "\n",
    "<img src=\"resources/mse.png\" alt=\"MSE Springs\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Minimising the Error Function in Matrix Form\n",
    "\n",
    "Now, to get a good approximation, we need to select weights $\\mathbf{w}$ so that the error $J(\\mathbf{w})$ is minimised. This is commonly called *ordinary least squares* or OLS. There are several ways to do this, for example, gradient descent, however, for now we will simply take the derivative of $J(\\mathbf{w})$ with respect to $\\mathbf{w}$ and\n",
    "then equate it to zero to get the closed-form solution.\n",
    "\n",
    "First though, we need to expand the mean squared error representation so that we can differentiate it. The constant $\\frac{1}{N}$ has been removed as it will not impact the selected weights.\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "J(\\mathbf{w}) &= \\lVert \\mathbf{X}\\mathbf{w} -\n",
    "\\mathbf{y}\\rVert^2 \\\\\n",
    "& = (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^\\intercal(\\mathbf{X}\\mathbf{w} -\n",
    "\\mathbf{y}) \\\\\n",
    "& = ((\\mathbf{X}\\mathbf{w})^\\intercal - \\mathbf{y}^\\intercal)(\\mathbf{X}\n",
    "\\mathbf{w} - \\mathbf{y}) \\\\\n",
    "& = (\\mathbf{X}\\mathbf{w})^\\intercal \\mathbf{X}\\mathbf{w} -\n",
    "(\\mathbf{X}\\mathbf{w})^\\intercal \\mathbf{y} - \\mathbf{y}^\\intercal(\\mathbf{X}\n",
    "\\mathbf{w}) + \\mathbf{y}^\\intercal\\mathbf{y} \\\\\n",
    "& = \\mathbf{w}^\\intercal\\mathbf{X}^\\intercal\\mathbf{X}\\mathbf{w} -\n",
    "2(\\mathbf{X}\\mathbf{w})^\\intercal \\mathbf{y} + \\mathbf{y}^\\intercal\\mathbf{y} \\\\\n",
    "& = \\mathbf{w}^\\intercal\\mathbf{X}^\\intercal\\mathbf{X}\\mathbf{w} -\n",
    "2\\mathbf{y}^\\intercal\\mathbf{X}\\mathbf{w} + \\mathbf{y}^\\intercal\\mathbf{y}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "  Before we move on, here are some useful properties for matrix differentiation:\n",
    "  <ul>\n",
    "    <li>$\\frac{\\partial \\mathbf{w}^\\intercal\\mathbf{A}\\mathbf{w}}{\\partial \\mathbf{w}} = 2\\mathbf{A}^\\intercal\\mathbf{w}$</li>\n",
    "  </ul>\n",
    "  <ul>\n",
    "    <li>$\\frac{\\partial \\mathbf{B}\\mathbf{w}}{\\partial \\mathbf{w}} = \\mathbf{B}^\\intercal$</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "Let $A = \\mathbf{X}^\\intercal\\mathbf{X}$ and $B = 2\\mathbf{y}^\\intercal\\mathbf{X}$. Substitute and differentiate:\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}}\n",
    "&= \\frac{\\partial}{\\partial \\mathbf{w}}\n",
    "(\\mathbf{w}^\\intercal\\mathbf{X}^\\intercal\\mathbf{X}\\mathbf{w} -\n",
    "2\\mathbf{y}^\\intercal\\mathbf{X}\\mathbf{w} +\n",
    "\\mathbf{y}^\\intercal\\mathbf{y}) \\\\\n",
    "&= \\frac{\\partial}{\\partial \\mathbf{w}}\n",
    "(\\mathbf{w}^\\intercal A \\mathbf{w} -\n",
    "B\\mathbf{w} +\n",
    "\\mathbf{y}^\\intercal\\mathbf{y}) \\\\\n",
    "&= 2\\mathbf{A}^\\intercal\\mathbf{w} - \\mathbf{B}^\\intercal + 0\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "\n",
    "Now, let's replace $\\mathbf{A}$ and $\\mathbf{B}$:\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}}\n",
    "= 2\\mathbf{X}^\\intercal\\mathbf{X}\\mathbf{w} - 2\\mathbf{X}^\\intercal\\mathbf{y}\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "\n",
    "Finally, let's throw away constant terms, equate to zero, and solve for $\\mathbf{w}$:\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}}\n",
    "&= 0 \\\\\n",
    "\\mathbf{X}^\\intercal\\mathbf{X}\\mathbf{w} - \\mathbf{X}^\\intercal\\mathbf{y} &= 0 \\\\\n",
    "\\mathbf{X}^\\intercal\\mathbf{X}\\mathbf{w} &= \\mathbf{X}^\\intercal\\mathbf{y} \\\\\n",
    "\\mathbf{w} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "\n",
    "And there we have it, the closed-form solution for ordinary least squares.\n",
    "\n",
    "Notice how we have to compute the inverse of a matrix. This means that $\\mathbf{X}^\\intercal\\mathbf{X}$ must be non-singular, however, there are ways to circumvent this issue, for example, by using the Moore-Penrose pseudoinverse instead: `numpy.linalg.pinv()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Closed-Form Solution\n",
    "\n",
    "To use the closed-form solution we derived above to solve the `income` vs. `education` problem we require a few things, namely:\n",
    "\n",
    "* The design matrix $\\mathbf{X}$\n",
    "* A column vector of ground truths $\\mathbf{y}$\n",
    "* A function that takes the two aforementioned matrices and evaluates the closed-form solution to get a set of weights $\\mathbf{w}$\n",
    "\n",
    "The last two requirements will have to be implemented by you.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>In the follow code snippet we will:</strong>\n",
    "  <ul>\n",
    "    <li>Create the design matrix $\\mathbf{X}$</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X(x_data):\n",
    "    \"\"\"Return design matrix given an array of N samples with d dimensions.\n",
    "    \"\"\"\n",
    "    # Create matrix Ax1 if d = 1\n",
    "    if x_data.ndim == 1:\n",
    "        x_data = np.expand_dims(x_data, axis=1)\n",
    "\n",
    "    # Find the number of samples and dimensions\n",
    "    nb_samples = x_data.shape[0]\n",
    "    nb_dimensions = x_data.shape[1]\n",
    "\n",
    "    # Create Nxd+1 matrix filled with ones\n",
    "    _X = np.ones((nb_samples, nb_dimensions + 1))\n",
    "\n",
    "    # Paste in the data we have in the new matrix\n",
    "    _X[:nb_samples, 1:nb_dimensions + 1] = x_data\n",
    "\n",
    "    return _X\n",
    "\n",
    "# Test and see that the design matrix was built correctly\n",
    "tests.test_build_x(build_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task I: Build y\n",
    "\n",
    "The second component we require is the vector $\\mathbf{y}$. This is a column vector over all the ground truths or target values in our training dataset. For completeness, it has the following form:\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{y} = \\left[ \\begin{array}{c} y_{1} \\\\ y_{2} \\\\ \\vdots\\\\ y_{N}\\end{array} \\right]\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "**Task**: Build the $\\mathbf{y}$ vector shown above. Use the previous code snippet as a reference for your implementation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def build_y(y_data):\n",
    "    \"\"\"Return a column vector containing the target values y.\n",
    "    \"\"\"\n",
    "    # Make a copy of the argument that we can work on\n",
    "    _y = y_data.copy()\n",
    "\n",
    "    # Create y matrix Nx1\n",
    "\n",
    "\n",
    "    # Return result\n",
    "    return _y\n",
    "\n",
    "### Do *not* modify the following line ###\n",
    "# Test and see that the y vector was built correctly\n",
    "tests.test_build_y(build_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task II: Implement Closed-Form Solution\n",
    "\n",
    "Now that we have both the design matrix $\\mathbf{X}$ and the vector of target values $\\mathbf{y}$ we can fit a linear model using the closed-form solution we derived before. Remember all of we have to do is implement the following expression:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{w} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Please refer to the following sources for how to utilise the various functions in NumPy when implementing your solution:\n",
    "\n",
    "* How to perform matrix multiplication in NumPy [np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)\n",
    "* How to compute the inverse of a matrix in NumPy [np.linalg.inv()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) or [np.linalg.pinv()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html)\n",
    "* How to transpose a NumPy array [X.T](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.T.html)\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "**Task**: Implement a function that evaluates the closed-form solution given a design matrix $\\mathbf{X}$ and target vector $\\mathbf{y}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(X, y):\n",
    "    \"\"\"Return a vector of weights found by the derived closed-form solution.\n",
    "    \"\"\"\n",
    "    weights = None\n",
    "\n",
    "    # Implement closed-form solution here\n",
    "\n",
    "    \n",
    "    return weights\n",
    "\n",
    "### Do *not* modify the following line ###\n",
    "# Test and see that the weights are calculated correctly\n",
    "tests.test_compute_theta(compute_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task III: Learn a Linear Regression Model\n",
    "\n",
    "We have now implemeted all of the necessary building blocks:\n",
    "\n",
    "* `build_X()` : Used to build the design matrix $\\mathbf{X}$\n",
    "* `build_y()` : Used to build the vector of target values $\\mathbf{y}$\n",
    "* `compute_weights` : Used to fit a linear model to the data using the solution we derived above\n",
    "\n",
    "After we have estimated $\\mathbf{w}$ we can perform predictions on unseen data by computing: $\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w}$.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "**Task**: Learn the weights $\\mathbf{w}$ given the building blocks we have implemented.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build design matrix (TASK)\n",
    "X = None\n",
    "\n",
    "# Build y vector (TASK)\n",
    "y = None\n",
    "\n",
    "# Learn linear model (TASK)\n",
    "W = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>In the follow code snippet we will:</strong>\n",
    "  <ul>\n",
    "    <li>Print the weights we learned</li>\n",
    "    <li>Plot the hyperplane (line in our case because $d=1$) that $\\mathbf{w}$ represents</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print weights\n",
    "print('The learned linear model looks like this:')\n",
    "print('Y = {:.3f} x + {:.3f}'.format(W[1, 0], W[0, 0]))\n",
    "\n",
    "# Plot hyperplane and training data\n",
    "xs = np.linspace(X_data.min(), X_data.max(), num=50)\n",
    "ys = np.dot(build_X(xs), W)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(X_data, y_data, label='Training data')\n",
    "plt.plot(xs, ys, color='Red', linewidth=1, label='Fit')\n",
    "\n",
    "plt.title('Education vs. Income')\n",
    "plt.xlabel('Education index')\n",
    "plt.ylabel('Income index')\n",
    "plt.grid(linestyle='dotted')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Critical Analysis\n",
    "\n",
    "Albeit easy to derive and easy to use, our closed-form solution has a few shortcomings:\n",
    "\n",
    "* Requires matrix inversion\n",
    "    * Very computationally expensive\n",
    "    * Not ideal for distributed computing\n",
    "* Issues become apparant when the number of features $d$ and number of samples $N$ begin to grow\n",
    "    * Depending on the size of the dataset it might be difficult / infeasible to fit all of it in memory\n",
    "\n",
    "To tackle these issues we will attempt to solve the linear regression problem using an iterative optimisation method called **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "In artificial neural network literature one can see several different symbols in use to signify the error function. For example, in addition to $J$ there is also $E$ (error), $L$ (loss), $C$ (cost), and even $err$. The rest of this notebook will use the symbol $E$ instead of $J$.\n",
    "</div>\n",
    "\n",
    "Gradient descent is an iterative optimisation algorithm. In general, it works by taking the derivative of an error function $E(\\mathbf{w})$ with respect to the parameters $\\mathbf{w}$ of the model, and then alter the parameters in the direction of the *negative* gradient.\n",
    "\n",
    "This can be summarised as: $\\mathbf{w}(k+1)\\leftarrow\\mathbf{w}(k) - \\eta\\frac{\\partial E(\\mathbf{w})}{\\partial\\mathbf{w}}$, where $\\mathbf{w}(k)$ signifies the state of the model parameters at iteration $k$, and $\\eta$ is known as the *learning rate* and decides how much the parameters should change with each application of the rule.\n",
    "\n",
    "This *update rule* is repeated until convergence or until the maximum number of iterations has been reached.\n",
    "\n",
    "**With gradient descent we can**:\n",
    "\n",
    "* Reduce memory issues by only working on parts of the data at a time\n",
    "* Distribute the computation among several computational nodes. This enables distributed computing and parallelisation which allows us to exploit new architectures such as GPUs, FPGAs, and ASICs\n",
    "* Gradient descent is a heavily use *type* of algorithm that opens the door for models such as artificial neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digression: A Different Perspective\n",
    "\n",
    "Linear models, such as linear regression, can be represented as artifical neural networks. An illustration of this can be seen below:\n",
    "\n",
    "<img src=\"resources/linear-regression-net.png\" alt=\"Linear regression as an artificial neural network\" width=\"300\" />\n",
    "\n",
    "As before, the input $\\mathbf{x} \\in \\mathbb{R}^d$ and the input is integrated via a linear combination plus a bias. The integrated value is activated by an activation function $\\sigma$, which for our linear regression model is defined as $\\sigma(x) = x$.\n",
    "\n",
    "In other words, $\\hat{y}$ is defined as $\\sigma(\\mathbf{X}\\mathbf{w})$, which simplifies to $\\mathbf{X}\\mathbf{w}$ because the activation function used for linear regression is the identity function. In artificial neural network terminology we would typically say that the activation function is *linear*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with Gradient Descent\n",
    "\n",
    "As we saw above, learning with gradient descent is easy. All we have to do is apply an *update rule* a set number of iterations until we are satisfied with the resulting weights. The update rule can be be seen below:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{w}(k+1)\\leftarrow\\mathbf{w}(k) - \\eta\\frac{\\partial E(\\mathbf{w})}{\\partial\\mathbf{w}}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "In words, the weights for the next iteration $k+1$ is the weights of the current iteration $k$ plus the *negative* gradient $\\frac{\\partial E(\\mathbf{w})}{\\partial\\mathbf{w}}$ scaled by the learning rate $\\eta$. In other words, for each iteration in gradient descent we adjust the weights we have with respect to the gradient of the error function $E(\\mathbf{w})$.\n",
    "\n",
    "An illustration of how this could look like with the mean squared error function can be seen below:\n",
    "\n",
    "<img src=\"resources/error-grad.png\" alt=\"MSE gradient\" width=\"300\" />\n",
    "\n",
    "The current state of several different weight states are signified by red dots, while the arrow points in the negative gradient direction. The optimal weight state is found at the minima, which yields the lowest amount of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Gradient\n",
    "\n",
    "To finalise the update rule we need to find: $\\frac{\\partial E(\\mathbf{w})}{\\partial\\mathbf{w}}$. This, of course, depends on the form of $E(\\mathbf{w})$.\n",
    "\n",
    "The squared error for a single sample $\\mathbf{x}_i$ in the training dataset is defined as:\n",
    "\n",
    "$E(\\mathbf{w}) = (\\hat{y}_i - y_i)^2$\n",
    "\n",
    "where $\\hat{y}_i=\\sigma(g)$ and $g(\\mathbf{x})=\\mathbf{w}^\\intercal \\mathbf{x}_i$.\n",
    "\n",
    "To simplify the derivation we will scale the squared error by halving it; this will not change the optimal solution:\n",
    "\n",
    "$E(\\mathbf{w}) = \\frac{1}{2}(\\hat{y}_i - y_i)^2$\n",
    "\n",
    "Let's now attempt to find the derivative we need:\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial\\mathbf{w}}\n",
    "= \\frac{\\partial}{\\partial\\mathbf{w}}( \\frac{1}{2}(\\hat{y}_i - y_i)^2)\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "\n",
    "Seeing as $\\hat{y}$ is dependent on $\\mathbf{w}$ we will need to use the chain rule of calculus.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Let $a(b) = \\frac{1}{2}(b)^2$ and $b(\\mathbf{w}) = (\\hat{y}_i - y_i)$, then $\\frac{\\partial a}{\\partial\\mathbf{w}}=\\frac{\\partial a}{\\partial b}\\frac{\\partial b}{\\partial\\mathbf{w}}$.\n",
    "</div>\n",
    "\n",
    "Therefore:\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial\\mathbf{w}}\n",
    "&= \\frac{\\partial a}{\\partial b}\\frac{\\partial b}{\\partial\\mathbf{w}} \\\\\n",
    "&= (\\hat{y}_i - y_i)\\frac{\\partial}{\\partial\\mathbf{w}}(\\hat{y}_i - y_i) \\\\\n",
    "&= (\\hat{y}_i - y_i)((\\frac{\\partial}{\\partial\\mathbf{w}}\\hat{y}_i) - (\\frac{\\partial}{\\partial\\mathbf{w}}y_i)) \\\\\n",
    "&= (\\hat{y}_i - y_i)((\\frac{\\partial}{\\partial\\mathbf{w}}\\hat{y}_i) - 0) \\\\\n",
    "&= (\\hat{y}_i - y_i)\\frac{\\partial}{\\partial\\mathbf{w}}\\hat{y}_i \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "\n",
    "Keep in mind that:\n",
    "\n",
    "* $\\hat{y}_i=\\sigma(g)$\n",
    "* $g(\\mathbf{x})=\\mathbf{w}^\\intercal \\mathbf{x}_i$.\n",
    "\n",
    "For now, let's replace $\\hat{y}$ with $\\sigma(g)$:\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial\\mathbf{w}}\n",
    "&= (\\hat{y}_i - y_i)\\frac{\\partial}{\\partial\\mathbf{w}}\\sigma(g)\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "\n",
    "Again we have to use the chain rule.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Let $a(b) = \\sigma(b)$ and $b(\\mathbf{w}) = (\\mathbf{w}^\\intercal \\mathbf{x}_i)$, then $\\frac{\\partial a}{\\partial\\mathbf{w}}=\\frac{\\partial a}{\\partial b}\\frac{\\partial b}{\\partial\\mathbf{w}}$.\n",
    "</div>\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial\\mathbf{w}}\n",
    "&= (\\hat{y}_i - y_i)\\frac{\\partial a}{\\partial b}\\frac{\\partial b}{\\partial\\mathbf{w}} \\\\\n",
    "&= (\\hat{y}_i - y_i)\\sigma '(g)\\mathbf{x}_i\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "\n",
    "Thus, the update rule for gradient descent, regardless of activation function, is defined as:\n",
    "\n",
    "<br class=\"math\" />\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{w}(k+1) \\leftarrow \\mathbf{w}(k) - \\eta((\\hat{y}_i - y_i)\\sigma '(g)\\mathbf{x}_i)\n",
    "\\end{equation*}\n",
    "$$\n",
    "<br class=\"math\" />\n",
    "\n",
    "Seeing as we're doing linear regression, we know that activation function is linear, i.e. $\\sigma(x)=x$, where $\\sigma'(x)=1$. So the final update rule will look like this:\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\mathbf{w}(k+1) &\\leftarrow \\mathbf{w}(k) - \\eta((\\hat{y}_i - y_i)\\mathbf{x}_i) \\\\\n",
    "&\\leftarrow \\mathbf{w}(k) - \\eta((\\mathbf{w}^\\intercal \\mathbf{x}_i - y_i)\\mathbf{x}_i)\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "Note that this updates the weights using only a single input example. This is generally called *stochastic* gradient descent. Typically the amount we adjust by is taken over a *batch*, i.e. subset, of examples.\n",
    "\n",
    "For completeness, the update rule above can be defined over a set of $m$ samples like so:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\mathbf{w}(k+1) \\leftarrow \\mathbf{w}(k) - \\eta\\frac{1}{m}\\sum_{i=i}^{m}(\\mathbf{w}^\\intercal \\mathbf{x}_i - y_i)\\mathbf{x}_i\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Keras\n",
    "\n",
    "Thankfully, when using gradient descent we do not need to derive and implement it ourselves as there are many programming libraries out there that can do automatic differentiation for us.\n",
    "\n",
    "In this and future notebooks we will be using the Python library [Keras](https://keras.io/). This is a high-level library for building and training artificial neural networks running on either [TensorFlow](https://www.tensorflow.org/) or [Theano](http://deeplearning.net/software/theano/). We will be able to leverage Keras when creating our linear regression model with gradient descent because linear models can be interpreted as artificial neural networks.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>In the following code snippets we will:</strong>\n",
    "  <ul>\n",
    "    <li>Create a linear regression model for the `Income` vs. `Education` problem in Keras</li>\n",
    "    <li>Train the model using (stochastic) gradient descent</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the modules we need from Keras as well as some additional ones we will use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# A library for easily displaying progress meters\n",
    "import tqdm\n",
    "\n",
    "# Contains all built-in optimisation tools in Keras, such as stochastic gradient descent\n",
    "from keras import optimizers\n",
    "\n",
    "# An input \"layer\" and a densely-connected neural network layer\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Model is an API that wraps our linear regression model\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to our model is a single scalar value (`Education`). The output is also a single scalar value (`Income`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is only a *single* feature\n",
    "input_X = Input(shape=(1,))\n",
    "\n",
    "# The output of the model is a single value\n",
    "output_y  = Dense(units=1, use_bias=True)(input_X)\n",
    "\n",
    "# We give the input and output to our Model API\n",
    "model = Model(inputs=input_X, outputs=output_y)\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the print above how the fully-connected layer `Dense()` has two *trainable* parameters. One is the weight (slope), while the second is the bias (intercept). Keras adds bias units by default, but it can be turned off by setting `use_bias=False`.\n",
    "\n",
    "The next thing we have to do in Keras is to set up an *optimiser* (sometimes called *solver*). There are many [alternatives](https://keras.io/optimizers/) to select from, however, we will settle for the stochastic gradient descent algorithm we discussed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Start by setting some user options\n",
    "#\n",
    "\n",
    "# Learning rate (set very small so we can clearly see the training progress)\n",
    "lr = 0.0001\n",
    "\n",
    "# Number of times to apply the update rule\n",
    "nb_iterations = 100\n",
    "\n",
    "# Number of samples to include each iteration (used to compute gradients)\n",
    "nb_samples = 30\n",
    "\n",
    "# Create optimiser using Keras\n",
    "sgd = optimizers.SGD(lr=lr)\n",
    "\n",
    "# Add the optimiser to our model, make it optimise mean squared error\n",
    "model.compile(optimizer=sgd, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that both the model definition and the optimiser is set up we can start training. Training using the Keras model API is done by calling the `fit()` method.\n",
    "\n",
    "Don't worry too much if this code is a bit too much right now. We will get much more experience with using Keras throughout the upcoming notebooks.\n",
    "\n",
    "While training the model, a plot is continuously updated to display the fitted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# Perform `nb_iterations` update rule applications\n",
    "for i in tqdm.tqdm(np.arange(nb_iterations)):\n",
    "    # Learn by calling the `fit` method\n",
    "    model.fit(X_data, y_data, \n",
    "              batch_size=nb_samples, \n",
    "              epochs=1, \n",
    "              verbose=0)\n",
    "\n",
    "    # Make a plot of the data and the current fit\n",
    "    xs = np.linspace(X_data.min(), X_data.max(), num=50)\n",
    "    ys = model.predict(xs)\n",
    "\n",
    "    ax.clear() \n",
    "\n",
    "    ax.scatter(X_data, y_data, label='Training data')\n",
    "    ax.plot(xs, ys, color='Red', linewidth=1, label='Fit')\n",
    "\n",
    "    ax.set_xlabel('Education index')\n",
    "    ax.set_ylabel('Income index')\n",
    "    ax.grid(linestyle='dotted')\n",
    "    ax.legend()\n",
    "\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
